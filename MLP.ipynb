{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_tutorial",
      "provenance": [],
      "authorship_tag": "ABX9TyPLUOaXhnicO/hJEik6JB/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PetchMa/deeplearning_fundamentals/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8r_OM1AKpm"
      },
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import jit, vmap, grad, pmap,value_and_grad\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umVgiRwCAcia",
        "outputId": "f2f23f20-19f6-4b7d-be06-ca161a905c4e"
      },
      "source": [
        "seed = 0\n",
        "\n",
        "def init_MLP(layer_widths,parent_key, scale =0.01):\n",
        "  params = []\n",
        "  keys = jax.random.split(parent_key,num=len(layer_widths)-1)\n",
        "  for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "    weight_key, bias_key = jax.random.split(key)\n",
        "\n",
        "    params.append(\n",
        "                   [scale*jax.random.normal(weight_key, shape=(out_width, in_width)),\n",
        "                    scale*jax.random.normal(bias_key, shape=(out_width,))]\n",
        "    )\n",
        "  return params\n",
        "\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "\n",
        "MLP_params = init_MLP([784, 512, 256, 10], rng)\n",
        "print(jax.tree_map(lambda x: x.shape, MLP_params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-9RzvXNDSV6",
        "outputId": "ceb27b3b-3453-4a1b-96d6-94c4694f931f"
      },
      "source": [
        "def MLP_predict(params, x):\n",
        "  hidden_layers = params[:-1]\n",
        "\n",
        "  activation = x\n",
        "  for w,b in hidden_layers:\n",
        "    activation = jax.nn.relu(jnp.dot(w,activation)+b)\n",
        "\n",
        "  w_last, b_last = params[-1]\n",
        "  logits = jnp.dot(w_last,activation)+b_last\n",
        "  return logits-logsumexp(logits) # basically does softmax lol but its log of softmax\n",
        "\n",
        "mnist_img_size = 784\n",
        "\n",
        "\n",
        "batched_MLP_predict = vmap(MLP_predict, in_axes=(None, 0))\n",
        "# small test\n",
        "dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))\n",
        "print(dummy_img_flat.shape)\n",
        "predictions = batched_MLP_predict(MLP_params, dummy_imgs_flat)\n",
        "print(predictions.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784,)\n",
            "(16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDaKyFIHEWR"
      },
      "source": [
        "# data loading\n",
        "def custom_transform(x):\n",
        "  return np.ravel(np.array(x, dtype=np.float32))\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download=True, transform=custom_transform)\n",
        "test_dataset = MNIST(root='train_mnist', train=False, download=True, transform=custom_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tT3Y-SoJQ8k",
        "outputId": "bfdca2ae-43a8-443a-83f4-5f0c480d044b"
      },
      "source": [
        "def custom_collate_fn(batch):\n",
        "  transposed_data = list(zip(*batch))\n",
        "  labels = np.array(transposed_data[1])\n",
        "  imgs = np.stack(transposed_data[0])\n",
        "  return imgs, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle = True, collate_fn=custom_collate_fn)\n",
        "batch_data = next(iter(train_loader))\n",
        "imgs = batch_data[0]\n",
        "labels = batch_data[1]\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z7bLSTpLXJB",
        "outputId": "82189504-d382-4fde-9036-5c31d2b97bfc"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "  predictions = batched_MLP_predict(params, imgs)\n",
        "  return -jnp.mean(predictions * gt_lbls)\n",
        "def update(params, imgs, gt_lbls, lr = 0.01):\n",
        "  loss, grads = value_and_grad(loss_fn)(params,imgs,gt_lbls)\n",
        "  return loss, jax.tree_multimap(lambda p,g:p-lr*g, params, grads)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  for count, (imgs, lbls) in enumerate(train_loader):\n",
        "    gt_labels = jax.nn.one_hot(lbls, len(MNIST.classes))\n",
        "    loss, MLP_params = update(MLP_params, imgs, gt_labels)\n",
        "    if count %10:\n",
        "      print(loss)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.036939897\n",
            "0.035040345\n",
            "0.03589448\n",
            "0.037989363\n",
            "0.031648155\n",
            "0.04111546\n",
            "0.030190075\n",
            "0.02479034\n",
            "0.030622883\n",
            "0.03551941\n",
            "0.029108495\n",
            "0.030718375\n",
            "0.04005232\n",
            "0.024839683\n",
            "0.02788696\n",
            "0.03585416\n",
            "0.03162801\n",
            "0.030825837\n",
            "0.020814296\n",
            "0.03646675\n",
            "0.032824684\n",
            "0.029956942\n",
            "0.026571726\n",
            "0.030289158\n",
            "0.034255087\n",
            "0.023487752\n",
            "0.03094072\n",
            "0.030177874\n",
            "0.034139518\n",
            "0.031127146\n",
            "0.041550975\n",
            "0.03225876\n",
            "0.026452316\n",
            "0.02748906\n",
            "0.033869576\n",
            "0.025060622\n",
            "0.036224402\n",
            "0.02608375\n",
            "0.0396219\n",
            "0.033136014\n",
            "0.023931673\n",
            "0.02887987\n",
            "0.026016075\n",
            "0.029170787\n",
            "0.026117254\n",
            "0.028350431\n",
            "0.030607313\n",
            "0.03254031\n",
            "0.04584183\n",
            "0.028943632\n",
            "0.026195599\n",
            "0.031373166\n",
            "0.025547868\n",
            "0.029473139\n",
            "0.029517261\n",
            "0.025783485\n",
            "0.04441143\n",
            "0.029654214\n",
            "0.028630568\n",
            "0.037721552\n",
            "0.024736708\n",
            "0.02483247\n",
            "0.025957445\n",
            "0.0323667\n",
            "0.026985884\n",
            "0.02844376\n",
            "0.026298268\n",
            "0.029461503\n",
            "0.026763955\n",
            "0.015076037\n",
            "0.027490968\n",
            "0.030276382\n",
            "0.02285366\n",
            "0.031382617\n",
            "0.022840772\n",
            "0.041468997\n",
            "0.016474327\n",
            "0.02953183\n",
            "0.032792505\n",
            "0.0372267\n",
            "0.023551175\n",
            "0.02310056\n",
            "0.025620788\n",
            "0.029785527\n",
            "0.036935512\n",
            "0.02544553\n",
            "0.027021378\n",
            "0.030700749\n",
            "0.029484754\n",
            "0.024035601\n",
            "0.03854419\n",
            "0.03688642\n",
            "0.026114557\n",
            "0.030374039\n",
            "0.03257932\n",
            "0.028301382\n",
            "0.038216975\n",
            "0.028202612\n",
            "0.03864031\n",
            "0.017350003\n",
            "0.031385843\n",
            "0.034623034\n",
            "0.020815955\n",
            "0.02471938\n",
            "0.022500973\n",
            "0.02590344\n",
            "0.02804104\n",
            "0.02386946\n",
            "0.029859057\n",
            "0.023946717\n",
            "0.014834649\n",
            "0.023888191\n",
            "0.015909081\n",
            "0.017573962\n",
            "0.020044511\n",
            "0.033745706\n",
            "0.03266975\n",
            "0.030846594\n",
            "0.026093233\n",
            "0.027531162\n",
            "0.027442945\n",
            "0.019973295\n",
            "0.023125513\n",
            "0.032785244\n",
            "0.026861062\n",
            "0.029590905\n",
            "0.025673276\n",
            "0.024552837\n",
            "0.02018473\n",
            "0.02830307\n",
            "0.019253952\n",
            "0.030163735\n",
            "0.033054397\n",
            "0.0324486\n",
            "0.032697808\n",
            "0.018500907\n",
            "0.015933247\n",
            "0.039220836\n",
            "0.025589323\n",
            "0.026664857\n",
            "0.032005265\n",
            "0.027601084\n",
            "0.03649777\n",
            "0.016262792\n",
            "0.017276088\n",
            "0.018513013\n",
            "0.023524895\n",
            "0.024150033\n",
            "0.027463917\n",
            "0.037488755\n",
            "0.018066458\n",
            "0.029349787\n",
            "0.037028197\n",
            "0.019374752\n",
            "0.026321148\n",
            "0.023491025\n",
            "0.03375876\n",
            "0.026744215\n",
            "0.032933753\n",
            "0.022900918\n",
            "0.02864486\n",
            "0.032705408\n",
            "0.038713798\n",
            "0.029501934\n",
            "0.019559244\n",
            "0.02160525\n",
            "0.026632905\n",
            "0.037631065\n",
            "0.03019059\n",
            "0.025463626\n",
            "0.036802858\n",
            "0.025846103\n",
            "0.015267253\n",
            "0.025352163\n",
            "0.031499658\n",
            "0.02771542\n",
            "0.0330755\n",
            "0.025273353\n",
            "0.032809112\n",
            "0.02581044\n",
            "0.015480054\n",
            "0.025637258\n",
            "0.02606428\n",
            "0.032227017\n",
            "0.030086068\n",
            "0.035212398\n",
            "0.03702998\n",
            "0.029948086\n",
            "0.026626116\n",
            "0.034025352\n",
            "0.016906047\n",
            "0.032291137\n",
            "0.027473522\n",
            "0.015603137\n",
            "0.023539681\n",
            "0.026065355\n",
            "0.021136118\n",
            "0.020555867\n",
            "0.020906497\n",
            "0.023268832\n",
            "0.029199729\n",
            "0.027227817\n",
            "0.021913169\n",
            "0.021623503\n",
            "0.027851833\n",
            "0.028123235\n",
            "0.03351747\n",
            "0.021252139\n",
            "0.036015954\n",
            "0.024235664\n",
            "0.018963214\n",
            "0.019634008\n",
            "0.042293873\n",
            "0.02362713\n",
            "0.028466245\n",
            "0.029820547\n",
            "0.022622138\n",
            "0.012932501\n",
            "0.014764425\n",
            "0.039104577\n",
            "0.022031682\n",
            "0.030292142\n",
            "0.01859567\n",
            "0.02290467\n",
            "0.04614598\n",
            "0.025654187\n",
            "0.021717926\n",
            "0.024758017\n",
            "0.027272547\n",
            "0.021390436\n",
            "0.025283176\n",
            "0.02385508\n",
            "0.024956023\n",
            "0.01923277\n",
            "0.023028797\n",
            "0.024016045\n",
            "0.039023615\n",
            "0.027850239\n",
            "0.032152675\n",
            "0.040211737\n",
            "0.021009522\n",
            "0.029744158\n",
            "0.02050585\n",
            "0.037758946\n",
            "0.022069803\n",
            "0.019190276\n",
            "0.029131621\n",
            "0.036282584\n",
            "0.019831156\n",
            "0.025118697\n",
            "0.021088267\n",
            "0.024128353\n",
            "0.027319936\n",
            "0.02608732\n",
            "0.03141914\n",
            "0.02547485\n",
            "0.020986035\n",
            "0.029588396\n",
            "0.023092462\n",
            "0.014551083\n",
            "0.019659448\n",
            "0.013359713\n",
            "0.03856386\n",
            "0.026620513\n",
            "0.021732049\n",
            "0.019427193\n",
            "0.017269885\n",
            "0.024139076\n",
            "0.02503777\n",
            "0.027024863\n",
            "0.034826\n",
            "0.018421244\n",
            "0.026456228\n",
            "0.037119545\n",
            "0.03269262\n",
            "0.024892142\n",
            "0.018584406\n",
            "0.01932135\n",
            "0.02551218\n",
            "0.029461553\n",
            "0.018351933\n",
            "0.024677828\n",
            "0.023407722\n",
            "0.018918673\n",
            "0.020170806\n",
            "0.026623849\n",
            "0.022275215\n",
            "0.02303082\n",
            "0.020546576\n",
            "0.023085615\n",
            "0.039323386\n",
            "0.01831665\n",
            "0.023033543\n",
            "0.024589239\n",
            "0.028572971\n",
            "0.019317374\n",
            "0.026469857\n",
            "0.021278683\n",
            "0.030617822\n",
            "0.04633962\n",
            "0.017737672\n",
            "0.020167265\n",
            "0.024235278\n",
            "0.025106728\n",
            "0.019149063\n",
            "0.03280325\n",
            "0.017582802\n",
            "0.030156797\n",
            "0.02242217\n",
            "0.020250821\n",
            "0.030135496\n",
            "0.025061652\n",
            "0.044555232\n",
            "0.023827586\n",
            "0.019713825\n",
            "0.031014645\n",
            "0.020482812\n",
            "0.028242081\n",
            "0.040518638\n",
            "0.020503724\n",
            "0.021295035\n",
            "0.027689591\n",
            "0.013613186\n",
            "0.02703\n",
            "0.040688075\n",
            "0.013163308\n",
            "0.033664413\n",
            "0.041000634\n",
            "0.027936524\n",
            "0.0131794335\n",
            "0.036646504\n",
            "0.03595045\n",
            "0.020912537\n",
            "0.024697373\n",
            "0.027129883\n",
            "0.028018916\n",
            "0.025817323\n",
            "0.031936448\n",
            "0.03565788\n",
            "0.021727318\n",
            "0.034301795\n",
            "0.034031387\n",
            "0.020885158\n",
            "0.031158805\n",
            "0.029948367\n",
            "0.02829866\n",
            "0.028636092\n",
            "0.027738323\n",
            "0.014123144\n",
            "0.03104367\n",
            "0.034630153\n",
            "0.020559376\n",
            "0.03447825\n",
            "0.026136076\n",
            "0.025792727\n",
            "0.027196014\n",
            "0.022316067\n",
            "0.026020069\n",
            "0.022486543\n",
            "0.032378975\n",
            "0.019681236\n",
            "0.028826056\n",
            "0.039626334\n",
            "0.021016253\n",
            "0.0198717\n",
            "0.020351937\n",
            "0.035369623\n",
            "0.028008545\n",
            "0.02353389\n",
            "0.025365716\n",
            "0.029865379\n",
            "0.028138494\n",
            "0.028820843\n",
            "0.01691565\n",
            "0.019451996\n",
            "0.022021918\n",
            "0.026041005\n",
            "0.020910218\n",
            "0.026929343\n",
            "0.03557122\n",
            "0.020076577\n",
            "0.028052999\n",
            "0.023224855\n",
            "0.02262\n",
            "0.02264346\n",
            "0.02124545\n",
            "0.026448408\n",
            "0.01933404\n",
            "0.025199592\n",
            "0.023467265\n",
            "0.03119186\n",
            "0.024898807\n",
            "0.019865865\n",
            "0.022542138\n",
            "0.03596362\n",
            "0.02604322\n",
            "0.024209732\n",
            "0.022946194\n",
            "0.019690244\n",
            "0.03574964\n",
            "0.036402896\n",
            "0.02277421\n",
            "0.030234834\n",
            "0.024326136\n",
            "0.023481065\n",
            "0.026094252\n",
            "0.02702159\n",
            "0.025182772\n",
            "0.012636292\n",
            "0.017996347\n",
            "0.023998115\n",
            "0.035568208\n",
            "0.027154488\n",
            "0.022719938\n",
            "0.012138088\n",
            "0.017449414\n",
            "0.029376227\n",
            "0.022618273\n",
            "0.027876273\n",
            "0.027156217\n",
            "0.03185929\n",
            "0.009375901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeFYLSQfODbX"
      },
      "source": [
        "def accuracy(params,loader):\n",
        "  for img, lbls in loader:\n",
        "    batched "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}