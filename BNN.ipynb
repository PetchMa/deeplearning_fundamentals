{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayes_nn",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMo8bZfrNleqGmTolj8ZPM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PetchMa/deeplearning_fundamentals/blob/main/BNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Bayes Neural Network\n",
        "In this notebook we will attempt to build a simple bayesian neural network from scratch using JAX and I will attempt to walk through the step process in doing so. \n",
        "\n",
        "First we import a number of packages, we will be using TORCH MNIST dataset as a starting ground as JAX doesnt handle data loading and all that. "
      ],
      "metadata": {
        "id": "dlwrQmpDA-_8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8r_OM1AKpm"
      },
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax import jit, vmap, grad, pmap,value_and_grad\n",
        "from time import time\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model\n",
        "\n",
        "We start off by building the individual models by creating effectively empty \"weights\". Recall what a Bayesian neural network __IS__. A Bayesian neural net seeks to learn no weight parameters but distributions, and thus for each parameter we have a mean AND a log normal standard deviation. Thus we implement the following: first a sampling function from a guassian. Then we need to wrap that function within a tree map as shown below:\n",
        "\n",
        "Notice now we have 4 parameters which we have to update, this is okay we just need to apply the correct feedforward implementation so our model will be split into two one section containing the mean ```mu``` and the other containing the standard deviation ```sigma``` which is stored together as a single dictionary!\n",
        "\n",
        "ALSO NOTE, we needed to use a variance of approx ~ 0.001 at the start or else this screws up the rest of  ~0.001 variance around default means that we need a LOG VARIANCE of -7 approx. "
      ],
      "metadata": {
        "id": "qFcyayS4BYO5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umVgiRwCAcia",
        "outputId": "e83f4864-ac69-4630-9060-e0596eb8f86a"
      },
      "source": [
        "seed = 0\n",
        "\n",
        "@jax.jit\n",
        "def sample_gaussian(mu, logvar, rng):\n",
        "  \"\"\"Sample from a Gaussian distribution.\n",
        "\n",
        "  NOTE: It uses reparameterization trick.\n",
        "  \"\"\"\n",
        "  eps = jax.random.normal(rng, shape=mu.shape)\n",
        "  return eps * jnp.exp(logvar / 2) + mu\n",
        "\n",
        "@jax.jit\n",
        "def sample_params(mu, sigma, rng):\n",
        "  sample = jax.tree_multimap(sample_gaussian, mu, sigma, rng)\n",
        "  return sample\n",
        "\n",
        "\n",
        "def init_MLP(layer_widths,parent_key, scale =0.01):\n",
        "  params = {}\n",
        "  mu = []\n",
        "  sigma =[]\n",
        "  keys = jax.random.split(parent_key,num=len(layer_widths)-1)\n",
        "  for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "    weight_key, bias_key = jax.random.split(key)\n",
        "    mu.append([scale*jax.random.normal(weight_key, shape=(out_width, in_width)),\n",
        "               scale*jax.random.normal(bias_key, shape=(out_width,))])\n",
        "    \n",
        "  params['mu'] = mu\n",
        "  #see we placed a log variance of -7 everywhere because ln(0.001)~ -7 \n",
        "  params['sigma'] = jax.tree_map(lambda x: -7 * jnp.ones_like(x), mu)\n",
        "\n",
        "\n",
        "  return params\n",
        "\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "\n",
        "MLP_params = init_MLP([784, 512, 256, 10], rng)\n",
        "#this checks the shape of the model\n",
        "print(jax.tree_map(lambda x: x.shape, MLP_params))\n",
        "print(type(MLP_params))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mu': [[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]], 'sigma': [[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed forward\n",
        "Now we want to make the model \"alive\" by implementing the actual feedforward aspect. Recall that feed forward is a simple linear combo of the model weights with some bias and the applied with a nonlinear activation function and passed on. \n",
        "\n",
        "However our neural network \"parameters\" describe an entire distribution of weights to choose from and thus we need to select those weights. \n",
        "\n",
        "The trick to remember is we can store the specific stuff within a dictionary within the layers. The issue with previous ML Libs is that these model structures are often abstracted away from us and makes it hard to manipulate the individual architecture. Now everything is up to you to implement correctly! \n",
        "\n"
      ],
      "metadata": {
        "id": "GeLlToWdCVYv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-9RzvXNDSV6",
        "outputId": "4e329b66-b170-496d-e498-c7f201bfd12d"
      },
      "source": [
        "from functools import partial \n",
        "@jax.jit\n",
        "def MLP_predict(paramas_key, x ):\n",
        "  params, rng = paramas_key\n",
        "  hidden_layers_mu = params['mu'][:-1]\n",
        "  hidden_layers_sigma = params['sigma'][:-1]\n",
        "  # rng=0\n",
        "  params_rng, rng = jax.random.split(rng)\n",
        "\n",
        "  activation = x\n",
        "  for mu, sigma in zip(hidden_layers_mu,hidden_layers_sigma) :\n",
        "    weight_mu = mu[0]\n",
        "    weight_sigma =  sigma[0]\n",
        "    bias_mu =  mu[1]\n",
        "    bias_sigma =  sigma[1]\n",
        "\n",
        "    w= sample_params(weight_mu, weight_sigma, params_rng)\n",
        "    b = sample_params(bias_mu, bias_sigma, params_rng)\n",
        "    activation = jax.nn.relu(jnp.dot(w,activation)+b)\n",
        "\n",
        "  hidden_layers_mu_last = params['mu'][-1]\n",
        "  hidden_layers_sigma_last = params['sigma'][-1]\n",
        "\n",
        "  weight_mu_last = hidden_layers_sigma_last[0]\n",
        "  weight_sigma_last =  hidden_layers_sigma_last[0]\n",
        "  bias_mu_last =  hidden_layers_mu_last[1]\n",
        "  bias_sigma_last =  hidden_layers_sigma_last[1]\n",
        "\n",
        "  w_last= sample_params(weight_mu_last, weight_sigma_last, params_rng)\n",
        "  b_last = sample_params(bias_mu_last, bias_sigma_last, params_rng)\n",
        "  logits = jnp.dot(w_last,activation)+b_last\n",
        "  return logits-logsumexp(logits) # basically does softmax lol but its log of softmax\n",
        "\n",
        "mnist_img_size = 784\n",
        "\n",
        "# LOOK AT THIS VMAP FUNCTION AND REMEMBER IT CLEARLY\n",
        "batched_MLP_predict = vmap(MLP_predict, in_axes=(None,0))\n",
        "# small test\n",
        "dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))\n",
        "\n",
        "predictions = batched_MLP_predict((MLP_params,rng), dummy_imgs_flat)\n",
        "print(predictions.shape)\n",
        "print(type(MLP_params))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 10)\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Transformation \n",
        "\n",
        "Okay this part is boring but we basically need to transform the images by squashing them down to vectors and then feeding it into the neural network. And so its not very interesting. Its just data augmentation stuff."
      ],
      "metadata": {
        "id": "90kSkYCkEjqD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDaKyFIHEWR"
      },
      "source": [
        "# data loading\n",
        "def custom_transform(x):\n",
        "  return np.ravel(np.array(x, dtype=np.float32))\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download=True, transform=custom_transform)\n",
        "test_dataset = MNIST(root='train_mnist', train=False, download=True, transform=custom_transform)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tT3Y-SoJQ8k",
        "outputId": "a22c8324-e9d0-483a-cd2a-c6a3f61492c8"
      },
      "source": [
        "def custom_collate_fn(batch):\n",
        "  transposed_data = list(zip(*batch))\n",
        "  labels = np.array(transposed_data[1])\n",
        "  imgs = np.stack(transposed_data[0])\n",
        "  return imgs, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle = True, collate_fn=custom_collate_fn)\n",
        "batch_data = next(iter(train_loader))\n",
        "imgs = batch_data[0]\n",
        "labels = batch_data[1]\n",
        "print(labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "Okay so another thing that is different about Bayes is that we need to compute what is called the evidence base lower bound or the ELBO function. Basically we just need to make sure the statistical difference between the layers needs to be great and the predictions need to be accurate. Thus we have to tack on this extra loss function. \n",
        "\n",
        "We compute the guassian kl divergence with the following function:"
      ],
      "metadata": {
        "id": "l6p54x8oXWxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def gaussian_kl(mu, logvar):\n",
        "    \"\"\"Computes mean KL between parameterized Gaussian and Normal distributions.\n",
        "\n",
        "    Gaussian parameterized by mu and logvar. Mean over the batch.\n",
        "\n",
        "    NOTE: See Appendix B from VAE paper (Kingma 2014):\n",
        "          https://arxiv.org/abs/1312.6114\n",
        "    \"\"\"\n",
        "    kl_divergence = jnp.sum(jnp.exp(logvar) + mu**2 - 1 - logvar) / 2\n",
        "    kl_divergence /= mu.shape[0]\n",
        "\n",
        "    return kl_divergence\n",
        "@jax.jit\n",
        "def elbo(params, imgs, gt_lbls,rng, beta=1):\n",
        "    predictions = batched_MLP_predict((params,rng), imgs)\n",
        "    # Compute log likelihood of batch.\n",
        "    log_likelihood = jnp.mean(predictions * gt_lbls)\n",
        "    # Compute the kl penalty on the approximate posterior.\n",
        "    kl_divergence = jax.tree_util.tree_reduce(\n",
        "        lambda a, b: a + b,\n",
        "        jax.tree_multimap(gaussian_kl,\n",
        "                          params['mu'],\n",
        "                          params['sigma']),\n",
        "    )\n",
        "    elbo_ = log_likelihood - beta * kl_divergence\n",
        "    return elbo_, log_likelihood, kl_divergence\n"
      ],
      "metadata": {
        "id": "GFPF1s7KXVHG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "Now to make the neural network learn we need a metric and this is the error function which is literally just the mean of the crossentropy error, since we've already gotten the log of the softmax we can just multiply it with the labels. note the labels are one-hot encodings and so  we dont need to loop through each nonzero value as those would get multiplied by 0. We can just take the average of this. We want to push this value up to 1. We basically did cross entropy loss\n",
        "\n",
        "We then want to update the neural network. Value and GRAD helps return both the actual function f(x) value and the grad gives the gradient of the function. \n",
        "\n",
        "Now this tree_multimap is a bit confusing... here is how it works basically the gradients returned are python treemaps which you can basically think of as like nested lists, but can take arbitrary data types and stuff. \n",
        "\n",
        "Look at the first section, we have ```lambda p,g:p-lr*g``` this is a simple for loop function that takes the parameter p, takes the corresponding gradient g and applies the stochastic gradient decent. with p = p-lr*g. Now to properly index the correct parameters and the correct gradients in respect to the parameters, we use the tree map. Since the parameters are stored in thes nested list structure, the gradients in respect to those layers are also stored in a similar fashion. Thus to \"unravel\" and apply back propagation we use the tree multi map\n",
        "\n",
        "However to test the actual accuracy we want to loop through multiple samplings of the data to get a reading on the final distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "7ltXLD22E0y6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z7bLSTpLXJB",
        "outputId": "d516e7ba-9098-40a4-842e-424e8927aefb"
      },
      "source": [
        "num_epochs = 100\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(logits, targets):\n",
        "    \"\"\"Returns classification accuracy.\"\"\"\n",
        "    # Return accuracy = how many predictions match the ground truth\n",
        "    return jnp.mean(jnp.argmax(logits, axis=-1) == targets)\n",
        "\n",
        "# @jax.jit\n",
        "def predict(params, batch_image, rng):\n",
        "    probs = []\n",
        "    num_samples = batch_image.shape[0]  \n",
        "    for i in range(num_samples):\n",
        "        params_rng, rng = jax.random.split(rng)\n",
        "        logits = batched_MLP_predict((params,params_rng), batch_image)\n",
        "        probs.append(jax.nn.softmax(logits))\n",
        "    stack_probs = jnp.stack(probs)\n",
        "    return jnp.mean(stack_probs, axis=0), jnp.std(stack_probs, axis=0)\n",
        "\n",
        "@jax.jit\n",
        "def loss_fn(params, imgs, gt_lbls,rng):\n",
        "  return -elbo(params, imgs, gt_lbls, rng)[1]\n",
        "\n",
        "# @jax.jit\n",
        "# def loss_fn(params, imgs, gt_lbls,rng):\n",
        "#   return -elbo(params, imgs, gt_lbls, rng)[0]\n",
        "\n",
        "@jax.jit\n",
        "def update(params, imgs, gt_lbls, rng, lr = 0.01):\n",
        "  loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls, rng)\n",
        "  return loss, jax.tree_multimap(lambda p, g: p - lr * g, params, grads)\n",
        "\n",
        "\n",
        "\n",
        "MLP_params = init_MLP([784, 512, 256, 10], rng,scale=0.01)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  start = time()\n",
        "  for count, (imgs, lbls) in enumerate(train_loader):\n",
        "    gt_labels = jax.nn.one_hot(lbls, len(MNIST.classes))\n",
        "    loss, MLP_params = update(MLP_params, imgs, gt_labels,rng)\n",
        "    if epochs %10==0 and count==0:\n",
        "      mean, var = predict(MLP_params, imgs, rng)\n",
        "      classes = jnp.argmax(gt_labels, axis=-1)\n",
        "      acc = accuracy(mean, classes)\n",
        "      print(epochs, 'LOSS: ',loss,\" ACCURACY\", acc, \" time taken: \", round((time()-start)/60,5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 LOSS:  2412058.8  ACCURACY 0.1328125  time taken:  0.08032\n",
            "10 LOSS:  nan  ACCURACY 0.046875  time taken:  0.00676\n",
            "20 LOSS:  nan  ACCURACY 0.0625  time taken:  0.00691\n",
            "30 LOSS:  nan  ACCURACY 0.078125  time taken:  0.00637\n",
            "40 LOSS:  nan  ACCURACY 0.1015625  time taken:  0.0063\n",
            "50 LOSS:  nan  ACCURACY 0.125  time taken:  0.00628\n"
          ]
        }
      ]
    }
  ]
}